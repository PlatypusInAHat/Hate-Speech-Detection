{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f108344",
   "metadata": {},
   "source": [
    "# Hate Speech Detection — Colab Notebook (Improved)\n",
    "Notebook này tự chạy trên **Google Colab** cho dự án Hate Speech.\n",
    "**Tính năng chính**:\n",
    "- Tự động đọc dữ liệu từ `data/` (hoặc upload zip)\n",
    "- Chuẩn hoá cột (`text`, `label`)\n",
    "- Huấn luyện **vinai/phobert-base** (mặc định) với **EarlyStopping**\n",
    "- Tính **accuracy / precision / recall / f1** và hiển thị **confusion matrix**\n",
    "- Hỗ trợ **class weights** để xử lý mất cân bằng\n",
    "\n",
    "> Nếu gặp lỗi tải PhoBERT, đổi sang `bert-base-multilingual-cased`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a505d818",
   "metadata": {},
   "source": [
    "## 0) Kiểm tra GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c810bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch, sys, platform\n",
    "print(\"PyTorch:\", torch.__version__, \"Python:\", sys.version.split()[0])\n",
    "!nvidia-smi || echo \"No GPU? Vào Runtime ▸ Change runtime type ▸ chọn GPU\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed15ebb9",
   "metadata": {},
   "source": [
    "## 1) Cài thư viện cần thiết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0c9117",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip -q install -U transformers datasets evaluate accelerate scikit-learn matplotlib\n",
    "# Tắt Weights & Biases để tránh prompt đăng nhập\n",
    "import os\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c767a9",
   "metadata": {},
   "source": [
    "## 2) Lấy dữ liệu\n",
    "### Cách A (khuyến nghị): Upload zip dự án (chứa thư mục `hate speech/`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7ce8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from google.colab import files\n",
    "import zipfile, io, os, shutil\n",
    "\n",
    "UPLOAD_DIR = \"/content/project\"\n",
    "os.makedirs(UPLOAD_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Chọn file zip dự án (ví dụ: hate speech (3).zip) ...\")\n",
    "uploaded = files.upload()\n",
    "for name, data in uploaded.items():\n",
    "    if name.endswith(\".zip\"):\n",
    "        with zipfile.ZipFile(io.BytesIO(data), 'r') as zf:\n",
    "            zf.extractall(UPLOAD_DIR)\n",
    "        print(\"Đã giải nén vào:\", UPLOAD_DIR)\n",
    "\n",
    "# Đoán thư mục gốc dự án\n",
    "# Tìm thư mục có tên 'hate speech'\n",
    "import glob\n",
    "candidates = glob.glob(UPLOAD_DIR + \"/**/hate speech\", recursive=True)\n",
    "PROJECT_DIR = candidates[0] if candidates else UPLOAD_DIR  # fallback\n",
    "print(\"PROJECT_DIR =\", PROJECT_DIR)\n",
    "\n",
    "DATA_DIR = PROJECT_DIR + \"/data\"\n",
    "print(\"DATA_DIR =\", DATA_DIR)\n",
    "!ls -R \"$DATA_DIR\" || echo \"Không tìm thấy thư mục data/. Hãy tự đặt đường dẫn DATA_DIR.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa26e32d",
   "metadata": {},
   "source": [
    "### Cách B: Mount Google Drive (tuỳ chọn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d144ab11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# PROJECT_DIR = \"/content/drive/MyDrive/your_project/hate speech\"\n",
    "# DATA_DIR = PROJECT_DIR + \"/data\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1ce9c3",
   "metadata": {},
   "source": [
    "## 3) Đọc & chuẩn hoá dữ liệu (`text`, `label`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f96f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, pandas as pd, numpy as np\n",
    "\n",
    "def _load_csv(path):\n",
    "    df = pd.read_csv(path)\n",
    "    # chuẩn hoá tên cột\n",
    "    cols = {c.lower().strip(): c for c in df.columns}\n",
    "    # map synonym -> canonical\n",
    "    if \"text\" not in cols and \"free_text\" in cols:\n",
    "        df.rename(columns={cols[\"free_text\"]:\"text\"}, inplace=True)\n",
    "    if \"label\" not in cols and \"label_id\" in cols:\n",
    "        df.rename(columns={cols[\"label_id\"]:\"label\"}, inplace=True)\n",
    "    # Nếu vẫn chưa có 'text' hoặc 'label', thử suy đoán\n",
    "    if \"text\" not in df.columns:\n",
    "        # chọn cột string dài nhất\n",
    "        str_cols = [c for c in df.columns if df[c].dtype == object]\n",
    "        if str_cols:\n",
    "            df.rename(columns={str_cols[0]:\"text\"}, inplace=True)\n",
    "    if \"label\" not in df.columns:\n",
    "        # nếu còn cột duy nhất kiểu int, dùng làm label\n",
    "        int_cols = [c for c in df.columns if np.issubdtype(df[c].dtype, np.integer)]\n",
    "        if int_cols:\n",
    "            df.rename(columns={int_cols[0]:\"label\"}, inplace=True)\n",
    "    keep = [c for c in [\"text\",\"label\"] if c in df.columns]\n",
    "    return df[keep]\n",
    "\n",
    "def try_paths(root):\n",
    "    # ưu tiên data/ gốc\n",
    "    cands = [\n",
    "        (root + \"/train.csv\", root + \"/dev.csv\", root + \"/test.csv\"),\n",
    "        (root + \"/vihsd/train.csv\", root + \"/vihsd/dev.csv\", root + \"/vihsd/test.csv\"),\n",
    "    ]\n",
    "    for tr, dv, te in cands:\n",
    "        if os.path.exists(tr) and os.path.exists(dv) and os.path.exists(te):\n",
    "            return tr, dv, te\n",
    "    raise FileNotFoundError(\"Không tìm thấy bộ 3 file train/dev/test trong data/ hoặc data/vihsd/.\")\n",
    "\n",
    "train_path, dev_path, test_path = try_paths(DATA_DIR)\n",
    "train_df, dev_df, test_df = _load_csv(train_path), _load_csv(dev_path), _load_csv(test_path)\n",
    "print(train_df.shape, dev_df.shape, test_df.shape)\n",
    "print(train_df.head(3))\n",
    "print(dev_df.head(3))\n",
    "\n",
    "# Làm sạch cơ bản\n",
    "def clean_text(s):\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    return \" \".join(s.split())  # gọn khoảng trắng\n",
    "for df in (train_df, dev_df, test_df):\n",
    "    df[\"text\"] = df[\"text\"].astype(str).map(clean_text)\n",
    "    df.dropna(subset=[\"text\",\"label\"], inplace=True)\n",
    "\n",
    "# Đảm bảo label là int (0..K-1)\n",
    "if not np.issubdtype(train_df[\"label\"].dtype, np.integer):\n",
    "    # nếu label ở dạng chuỗi, map sang số\n",
    "    uniq = sorted(train_df[\"label\"].astype(str).unique().tolist())\n",
    "    label2id = {v:i for i,v in enumerate(uniq)}\n",
    "    id2label = {i:v for v,i in label2id.items()}\n",
    "    for df in (train_df, dev_df, test_df):\n",
    "        df[\"label\"] = df[\"label\"].astype(str).map(label2id)\n",
    "else:\n",
    "    # lấy id2label theo unique từ train\n",
    "    uniq = sorted(train_df[\"label\"].unique().tolist())\n",
    "    label2id = {int(i):int(i) for i in uniq}\n",
    "    id2label = {int(i):str(i) for i in uniq}\n",
    "\n",
    "num_labels = len(set(train_df[\"label\"])) \n",
    "print(\"num_labels =\", num_labels)\n",
    "print(\"label2id:\", label2id)\n",
    "\n",
    "# Lưu tạm sau chuẩn hoá (tham khảo)\n",
    "TMP_DIR = \"/content/tmp_processed\"\n",
    "os.makedirs(TMP_DIR, exist_ok=True)\n",
    "train_df.to_csv(TMP_DIR + \"/train.csv\", index=False)\n",
    "dev_df.to_csv(TMP_DIR + \"/dev.csv\", index=False)\n",
    "test_df.to_csv(TMP_DIR + \"/test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183a6771",
   "metadata": {},
   "source": [
    "## 4) Khám phá nhanh (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783be84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_dist(df, title):\n",
    "    ax = df['label'].value_counts().sort_index().plot(kind='bar', rot=0)\n",
    "    ax.set_title(title); ax.set_xlabel(\"label\"); ax.set_ylabel(\"count\")\n",
    "    plt.show()\n",
    "\n",
    "plot_dist(train_df, \"Train label distribution\")\n",
    "plot_dist(dev_df,   \"Dev label distribution\")\n",
    "plot_dist(test_df,  \"Test label distribution\")\n",
    "print(\"Ví dụ mẫu:\n",
    "\", train_df.sample(3, random_state=42))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bee17ae",
   "metadata": {},
   "source": [
    "## 5) Tokenizer & HuggingFace Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafe995a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"vinai/phobert-base\"  # đổi nếu cần\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        padding=False,\n",
    "    )\n",
    "\n",
    "hf_ds = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df, preserve_index=False),\n",
    "    \"validation\": Dataset.from_pandas(dev_df, preserve_index=False),\n",
    "    \"test\": Dataset.from_pandas(test_df, preserve_index=False),\n",
    "}).map(tokenize_fn, batched=True)\n",
    "\n",
    "hf_ds = hf_ds.remove_columns([c for c in hf_ds[\"train\"].column_names if c not in (\"input_ids\",\"attention_mask\",\"label\")])\n",
    "hf_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef27cbd3",
   "metadata": {},
   "source": [
    "## 6) Huấn luyện — Trainer + Class Weights + EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18f75a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np, torch\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding, EarlyStoppingCallback\n",
    "import evaluate\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=num_labels, label2id={str(k):k for k in range(num_labels)}, id2label={k:str(k) for k in range(num_labels)}\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "acc_metric  = evaluate.load(\"accuracy\")\n",
    "prec_metric = evaluate.load(\"precision\")\n",
    "rec_metric  = evaluate.load(\"recall\")\n",
    "f1_metric   = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\":  acc_metric.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"precision\": prec_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"precision\"],\n",
    "        \"recall\":    rec_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"recall\"],\n",
    "        \"f1\":        f1_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"],\n",
    "    }\n",
    "\n",
    "# Class weights (optional)\n",
    "USE_CLASS_WEIGHTS = True\n",
    "class_weights = None\n",
    "if USE_CLASS_WEIGHTS:\n",
    "    y = train_df[\"label\"].values\n",
    "    classes = np.unique(y)\n",
    "    weights = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y)\n",
    "    class_weights = torch.tensor(weights, dtype=torch.float)\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**{k: v for k,v in inputs.items() if k != \"labels\"})\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights.to(logits.device) if class_weights is not None else None)\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"/content/results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    logging_steps=50,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    gradient_accumulation_steps=1,\n",
    "    report_to=\"none\",\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=hf_ds[\"train\"],\n",
    "    eval_dataset=hf_ds[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31241767",
   "metadata": {},
   "source": [
    "## 7) Đánh giá trên Test + Ma trận nhầm lẫn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a6fc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "pred = trainer.predict(hf_ds[\"test\"])\n",
    "y_true = pred.label_ids\n",
    "y_pred = np.argmax(pred.predictions, axis=-1)\n",
    "\n",
    "print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion matrix:\\n\", cm)\n",
    "\n",
    "# plot\n",
    "import itertools\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.imshow(cm, interpolation='nearest')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(num_labels)\n",
    "plt.xticks(tick_marks, [str(i) for i in range(num_labels)], rotation=45)\n",
    "plt.yticks(tick_marks, [str(i) for i in range(num_labels)])\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, format(cm[i, j], 'd'),\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bb3ec1",
   "metadata": {},
   "source": [
    "## 8) Lưu model lên Google Drive (tuỳ chọn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8545ec16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# SAVE_DIR = \"/content/drive/MyDrive/hate_speech_model_phobert\"\n",
    "# trainer.save_model(SAVE_DIR)\n",
    "# tokenizer.save_pretrained(SAVE_DIR)\n",
    "# print(\"Saved to\", SAVE_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43906e4",
   "metadata": {},
   "source": [
    "## 9) Suy luận nhanh (inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df744e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_texts(texts):\n",
    "    enc = tokenizer(texts, truncation=True, max_length=256, return_tensors=\"pt\").to(trainer.model.device)\n",
    "    with torch.no_grad():\n",
    "        logits = trainer.model(**enc).logits\n",
    "    preds = torch.argmax(logits, dim=-1).cpu().numpy().tolist()\n",
    "    return preds\n",
    "\n",
    "samples = [\n",
    "    \"Mày thật là đồ vô học!\",\n",
    "    \"Mọi người ơi, hãy giúp nhau và tử tế hơn nhé.\",\n",
    "]\n",
    "print(predict_texts(samples))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}