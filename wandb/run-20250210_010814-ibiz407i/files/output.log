C:\Users\BINH\AppData\Local\Programs\Python\Python311\Lib\site-packages\huggingface_hub\file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 557/557 [00:00<00:00, 769kB/s]
C:\Users\BINH\AppData\Local\Programs\Python\Python311\Lib\site-packages\huggingface_hub\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\Users\BINH\.cache\huggingface\hub\models--vinai--phobert-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.
To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development
  warnings.warn(message)
vocab.txt: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 895k/895k [00:01<00:00, 866kB/s]
bpe.codes: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 1.14M/1.14M [00:00<00:00, 1.66MB/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Map:  42%|████████████████████████████████████████▊                                                         | 10000/24048 [00:01<00:02, 5183.36 examples/s]
Traceback (most recent call last):
  File "D:\hate speech\src\train.py", line 34, in <module>
    dataset = load_and_prepare_datasets()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\hate speech\src\dataset.py", line 42, in load_and_prepare_datasets
    tokenized_datasets = dataset_dict.map(tokenize_function, batched=True)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\BINH\AppData\Local\Programs\Python\Python311\Lib\site-packages\datasets\dataset_dict.py", line 853, in map
    {
  File "C:\Users\BINH\AppData\Local\Programs\Python\Python311\Lib\site-packages\datasets\dataset_dict.py", line 854, in <dictcomp>
    k: dataset.map(
       ^^^^^^^^^^^^
  File "C:\Users\BINH\AppData\Local\Programs\Python\Python311\Lib\site-packages\datasets\arrow_dataset.py", line 592, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\BINH\AppData\Local\Programs\Python\Python311\Lib\site-packages\datasets\arrow_dataset.py", line 557, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\BINH\AppData\Local\Programs\Python\Python311\Lib\site-packages\datasets\arrow_dataset.py", line 3097, in map
    for rank, done, content in Dataset._map_single(**dataset_kwargs):
  File "C:\Users\BINH\AppData\Local\Programs\Python\Python311\Lib\site-packages\datasets\arrow_dataset.py", line 3474, in _map_single
    batch = apply_function_on_filtered_inputs(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\BINH\AppData\Local\Programs\Python\Python311\Lib\site-packages\datasets\arrow_dataset.py", line 3353, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\hate speech\src\dataset.py", line 35, in tokenize_function
    return tokenizer(
           ^^^^^^^^^^
  File "C:\Users\BINH\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\tokenization_utils_base.py", line 2561, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\BINH\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\tokenization_utils_base.py", line 2647, in _call_one
    return self.batch_encode_plus(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\BINH\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\tokenization_utils_base.py", line 2838, in batch_encode_plus
    return self._batch_encode_plus(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\BINH\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\tokenization_utils.py", line 733, in _batch_encode_plus
    first_ids = get_input_ids(ids)
                ^^^^^^^^^^^^^^^^^^
  File "C:\Users\BINH\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\tokenization_utils.py", line 713, in get_input_ids
    raise ValueError(
ValueError: Input is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.
